{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NwTKGwijo1s0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import difflib\n",
    "from collections import Counter\n",
    "import shutil\n",
    "import string\n",
    "import re\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Visualisation libraries\n",
    "## Text\n",
    "from colorama import Fore, Back, Style\n",
    "from IPython.display import Image, display, Markdown, Latex, clear_output\n",
    "\n",
    "## matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (17, 6)\n",
    "%matplotlib inline\n",
    "\n",
    "## seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "## plotly\n",
    "from plotly.offline import init_notebook_mode, iplot \n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "%config InlineBackend.figure_format = 'retina' \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Pipeline with Tensorflow\n",
    "\n",
    "This article is based on the Keras [**Text classification from scratch**](https://keras.io/examples/nlp/text_classification_from_scratch/) where we demonstrate a text classification pipeline using TensorFlow. The dataset used here is the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/) dataset from Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size=\"+2\"><b>\n",
    "Large Movie Review Dataset\n",
    "</b></font>\n",
    "</div>\n",
    "\n",
    "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. See the README file contained in the release for more details.\n",
    "\n",
    "## Downloading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Data(_URL, Remove = True):\n",
    "    # The dataset URL\n",
    "    File = _URL.split('/')[-1]\n",
    "    Full_Name =  os.path.join(os.getcwd(), File)\n",
    "    # Download the dataset file from the URL\n",
    "    path_to_zip = tf.keras.utils.get_file(fname =Full_Name, origin=_URL, extract=True, cache_dir = os.getcwd())\n",
    "    PATH = os.path.dirname(path_to_zip)\n",
    "    PATH = os.path.join(PATH, 'datasets')\n",
    "    Folder = difflib.get_close_matches(File.split('.')[0],os.listdir(PATH))\n",
    "    PATH = os.path.join(PATH, Folder[0])\n",
    "    # Deleting the zip file\n",
    "    if Remove:\n",
    "        os.remove(File)\n",
    "    return PATH\n",
    "    #-----------------------------------------------------------------\n",
    "    \n",
    "_URL = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "PATH = Get_Data(_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Directory Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[34m\u001b[22m========\u001b[0m\n",
      "\u001b[40m\u001b[36m\u001b[22maclImdb:\u001b[0m\n",
      "\u001b[0m\u001b[34m\u001b[22m========\u001b[0m\n",
      "└── \u001b[46m\u001b[30m\u001b[22mtest:\u001b[0m\n",
      "   └── \u001b[47m\u001b[30m\u001b[22mlabeledBow.feat\u001b[0m\n",
      "   └── \u001b[45m\u001b[30m\u001b[22mneg:\u001b[0m\n",
      "       \u001b[43m\u001b[30m\u001b[22m12500 TXT files\u001b[0m\n",
      "       0_2.txt, 10000_4.txt, 10001_1.txt, 10002_3.txt, 10003_3.txt, ...\n",
      "   └── \u001b[45m\u001b[30m\u001b[22mpos:\u001b[0m\n",
      "       \u001b[43m\u001b[30m\u001b[22m12500 TXT files\u001b[0m\n",
      "       0_10.txt, 10000_7.txt, 10001_9.txt, 10002_8.txt, 10003_8.txt, ...\n",
      "   └── \u001b[47m\u001b[30m\u001b[22murls_neg.txt\u001b[0m\n",
      "   └── \u001b[47m\u001b[30m\u001b[22murls_pos.txt\u001b[0m\n",
      "└── \u001b[46m\u001b[30m\u001b[22mtrain:\u001b[0m\n",
      "   └── \u001b[47m\u001b[30m\u001b[22mlabeledBow.feat\u001b[0m\n",
      "   └── \u001b[45m\u001b[30m\u001b[22mneg:\u001b[0m\n",
      "       \u001b[43m\u001b[30m\u001b[22m12500 TXT files\u001b[0m\n",
      "       0_3.txt, 10000_4.txt, 10001_4.txt, 10002_1.txt, 10003_1.txt, ...\n",
      "   └── \u001b[45m\u001b[30m\u001b[22mpos:\u001b[0m\n",
      "       \u001b[43m\u001b[30m\u001b[22m12500 TXT files\u001b[0m\n",
      "       0_9.txt, 10000_8.txt, 10001_10.txt, 10002_7.txt, 10003_8.txt, ...\n",
      "   └── \u001b[45m\u001b[30m\u001b[22munsup:\u001b[0m\n",
      "       \u001b[43m\u001b[30m\u001b[22m50000 TXT files\u001b[0m\n",
      "       0_0.txt, 10000_0.txt, 10001_0.txt, 10002_0.txt, 10003_0.txt, ...\n",
      "   └── \u001b[47m\u001b[30m\u001b[22munsupBow.feat\u001b[0m\n",
      "   └── \u001b[47m\u001b[30m\u001b[22murls_neg.txt\u001b[0m\n",
      "   └── \u001b[47m\u001b[30m\u001b[22murls_pos.txt\u001b[0m\n",
      "   └── \u001b[47m\u001b[30m\u001b[22murls_unsup.txt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def Path_Tree(PATH):\n",
    "    sep = ' ' * 3\n",
    "    title = PATH.split('\\\\')[-1]\n",
    "    print(Style.RESET_ALL + Fore.BLUE + Style.NORMAL + '=' * (len(title) +1) + Style.RESET_ALL)\n",
    "    print(Back.BLACK + Fore.CYAN + Style.NORMAL + title +':'+ Style.RESET_ALL)\n",
    "    print(Style.RESET_ALL + Fore.BLUE + Style.NORMAL + '=' * (len(title) +1)+ Style.RESET_ALL)\n",
    "    for entry in os.listdir(PATH):\n",
    "        sub = os.path.join(PATH, entry)\n",
    "        if os.path.isdir(sub):\n",
    "            print('└──',Back.CYAN + Fore.BLACK + Style.NORMAL + entry+':'+ Style.RESET_ALL)\n",
    "            for entry1 in os.listdir(sub):\n",
    "                sub1 = os.path.join(sub, entry1)\n",
    "                if os.path.isdir(sub):\n",
    "                    if os.path.isdir(sub1):\n",
    "                        print(sep + '└──',Back.MAGENTA + Fore.BLACK + Style.NORMAL + entry1+':'+ Style.RESET_ALL)\n",
    "                        List = os.listdir(sub1)\n",
    "                        print(2* sep, Back.YELLOW + Fore.BLACK + Style.NORMAL +\n",
    "                              '%i %s files' % (len(List), List[0].split('.')[-1].upper()) + Style.RESET_ALL)\n",
    "                        print(2* sep, ', '.join(List[:5]) + ', ...')\n",
    "                    else:\n",
    "                        print(sep + '└──',Back.WHITE + Fore.BLACK + Style.NORMAL + entry1+ Style.RESET_ALL)\n",
    "    #-----------------------------------------------------------------\n",
    "    \n",
    "Path_Tree(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove unnessary files and save the the address of folders that we need for our modeling and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Set</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Test</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Train</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subset</th>\n",
       "      <th>Neg</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "      <th>Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Size</th>\n",
       "      <td>12500</td>\n",
       "      <td>12500</td>\n",
       "      <td>12500</td>\n",
       "      <td>12500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Set      Test         Train       \n",
       "Subset    Neg    Pos    Neg    Pos\n",
       "Size    12500  12500  12500  12500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def Data_Info(PATH):\n",
    "    Set = [];\n",
    "    Subset = [];\n",
    "    Size = [];\n",
    "    DataDirs = {};\n",
    "    Temp = []\n",
    "    # Train and Test Sets subdirs\n",
    "    for entry in os.listdir(PATH):\n",
    "        sub = os.path.join(PATH, entry)\n",
    "        if os.path.isdir(sub):\n",
    "            DataDirs[entry] = sub\n",
    "            for entry1 in os.listdir(sub):\n",
    "                sub1 = os.path.join(sub, entry1)\n",
    "                if os.path.isdir(sub1):\n",
    "                    Temp.append(entry1)\n",
    "    Temp = Counter(Temp)\n",
    "    Temp = [x for x in Temp.keys() if Temp[x] ==2]\n",
    "    for entry in os.listdir(PATH):\n",
    "        sub = os.path.join(PATH, entry)\n",
    "        if os.path.isdir(sub):\n",
    "            DataDirs[entry] = sub\n",
    "            for entry1 in os.listdir(sub):\n",
    "                sub1 = os.path.join(sub, entry1)\n",
    "                if (os.path.isdir(sub1) & (entry1 in Temp)):\n",
    "                    DataDirs[entry + '_' +entry1] = sub1\n",
    "                    Set.append(entry.title())\n",
    "                    Subset.append(entry1.title())\n",
    "                    Size.append(len(os.listdir(sub1)))\n",
    "                else:\n",
    "                    try:\n",
    "                        os.remove(sub1)\n",
    "                    except:\n",
    "                        try:\n",
    "                            os.rmdir(sub1)\n",
    "                        except:\n",
    "                            shutil.rmtree(sub1)\n",
    "        else:\n",
    "            os.remove(sub)\n",
    "\n",
    "    DataFrame_Info = pd.DataFrame({'Set': Set, 'Subset': Subset, 'Size':Size})\n",
    "    display(DataFrame_Info.set_index(['Set' , 'Subset']).T)\n",
    "    return DataFrame_Info, DataDirs\n",
    "    #-----------------------------------------------------------------\n",
    "    \n",
    "DataFrame_Info, DataDirs = Data_Info(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[40m\u001b[32m\u001b[22mTrain Data:\u001b[0m \u001b[34m\u001b[22m============================================================================================================\u001b[0m\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Number of batches: 625\n",
      "\u001b[40m\u001b[32m\u001b[22mValidation Data:\u001b[0m \u001b[34m\u001b[22m=======================================================================================================\u001b[0m\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Number of batches: 157\n",
      "\u001b[40m\u001b[32m\u001b[22mTest Data:\u001b[0m \u001b[34m\u001b[22m=============================================================================================================\u001b[0m\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Number of batches: 782\n",
      "\u001b[34m\u001b[22m========================================================================================================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def Line(L=120): print(Fore.BLUE + Style.NORMAL + L*'=' + Style.RESET_ALL)\n",
    "\n",
    "def Header(Text='Title', L=120):\n",
    "    Text = Text + ':'\n",
    "    print(Back.BLACK + Fore.GREEN + Style.NORMAL + Text + Style.RESET_ALL + ' ' + Fore.BLUE +\n",
    "          Style.NORMAL +  (L- len(Text) - 1)*'=' + Style.RESET_ALL)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "batch_size = 32\n",
    "# Train Set: 80% Validation: 20%\n",
    "train_val_split_ratio = 0.2\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "Header('Train Data')\n",
    "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(DataDirs['train'], batch_size=batch_size,\n",
    "                                                                  validation_split = train_val_split_ratio,\n",
    "                                                                  subset=\"training\", seed=1337)\n",
    "print(\"Number of batches: %d\"% tf.data.experimental.cardinality(raw_train_ds))\n",
    "#\n",
    "Header('Validation Data')\n",
    "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(DataDirs['train'], batch_size = batch_size,\n",
    "                                                                validation_split=train_val_split_ratio,\n",
    "                                                                subset=\"validation\", seed=1337)\n",
    "print(\"Number of batches: %d\" % tf.data.experimental.cardinality(raw_val_ds))\n",
    "#\n",
    "Header('Test Data')\n",
    "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(DataDirs['test'],\n",
    "                                                                 batch_size=batch_size)\n",
    "print(\"Number of batches: %d\" % tf.data.experimental.cardinality(raw_test_ds))\n",
    "Line(120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XLKY3nqSo1tH"
   },
   "source": [
    "Let's preview a few samples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m\u001b[37m\u001b[22mA Positive Comment:\u001b[0m \u001b[32m\u001b[22m====================================================================================================\u001b[0m\n",
      "b'I\\'ve seen tons of science fiction from the 70s; some horrendously bad, and others thought provoking and truly frightening. Soylent Green fits into the latter category. Yes, at times it\\'s a little campy, and yes, the furniture is good for a giggle or two, but some of the film seems awfully prescient. Here we have a film, 9 years before Blade Runner, that dares to imagine the future as somthing dark, scary, and nihilistic. Both Charlton Heston and Edward G. Robinson fare far better in this than The Ten Commandments, and Robinson\\'s assisted-suicide scene is creepily prescient of Kevorkian and his ilk. Some of the attitudes are dated (can you imagine a filmmaker getting away with the \"women as furniture\" concept in our oh-so-politically-correct-90s?), but it\\'s rare to find a film from the Me Decade that actually can make you think. This is one I\\'d love to see on the big screen, because even in a widescreen presentation, I don\\'t think the overall scope of this film would receive its due. Check it out.'\n",
      "\u001b[32m\u001b[22m========================================================================================================================\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[41m\u001b[37m\u001b[22mA Negative Comment:\u001b[0m \u001b[31m\u001b[22m====================================================================================================\u001b[0m\n",
      "b\"I was talked into watching this movie by a friend who blubbered on about what a cute story this was.  Yuck.  I want my two hours back, as I could have done SO many more productive things with my time...like, for instance, twiddling my thumbs. I see nothing redeeming about this film at all, save for the eye-candy aspect of it...  3/10 (and that's being generous)\"\n",
      "\u001b[31m\u001b[22m========================================================================================================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def Comment_Sample(N = 2, L=120, ds = raw_train_ds, batch_size = batch_size):\n",
    "    Pos = []\n",
    "    Neg = []\n",
    "    for text_batch, label_batch in ds.take(1):\n",
    "        for i in range(batch_size):\n",
    "            if label_batch.numpy()[i] ==1:\n",
    "                Pos.append(i)\n",
    "            else:\n",
    "                Neg.append(i)\n",
    "        for i in Pos[:N]:\n",
    "            Text = 'A Positive Comment:'\n",
    "            print(Back.GREEN + Fore.WHITE + Style.NORMAL + Text + Style.RESET_ALL + ' ' + Fore.GREEN +\n",
    "                  Style.NORMAL +  (L- len(Text) - 1)*'=' + Style.RESET_ALL)\n",
    "            print(str(text_batch.numpy()[i]).replace(\"<br />\", \" \"))\n",
    "            print(Fore.GREEN + Style.NORMAL + L*'=' + Style.RESET_ALL)\n",
    "        print('\\n\\n')\n",
    "        for i in Neg[:N]:\n",
    "            Text = 'A Negative Comment:'\n",
    "            print(Back.RED + Fore.WHITE + Style.NORMAL + Text + Style.RESET_ALL + ' ' + Fore.RED +\n",
    "                  Style.NORMAL +  (L- len(Text) - 1)*'=' + Style.RESET_ALL)\n",
    "            print(str(text_batch.numpy()[i]).replace(\"<br />\", \" \"))\n",
    "            print(Fore.RED + Style.NORMAL + L*'=' + Style.RESET_ALL)\n",
    "            \n",
    "Comment_Sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size=\"+2\"><b>\n",
    "Modeling\n",
    "</b></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e1871GoXo1tJ"
   },
   "source": [
    "## Prepareprocessing\n",
    "\n",
    "Each comment contains a number of HTML breaks, `<br />`. To remove these substrings, we can use a variety of [tf.strings](https://www.tensorflow.org/api_docs/python/tf/strings ) module. In particular, we will use the following functions.\n",
    "\n",
    "| Function      | Description                                                                     |\n",
    "|---------------|---------------------------------------------------------------------------------|\n",
    "|     lower     | Converts all uppercase characters into their respective lowercase replacements. |\n",
    "| regex_replace |          Replace elements of input matching regex pattern with a rewrite.         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FilterData(Inp):\n",
    "    # Lowercase the data\n",
    "    Inp_low = tf.strings.lower(Inp)\n",
    "    # replacing \"<br />\" with a space character, \" \".\n",
    "    Out = tf.strings.regex_replace(Inp_low, \"<br />\", \" \")\n",
    "    # Removing punctuations\n",
    "    Out = tf.strings.regex_replace(Out, \"[%s]\" % re.escape(string.punctuation), \"\")\n",
    "    return Out\n",
    "\n",
    "# Model constants -----------------------------------------------------------------------------------------------\n",
    "Max_Features = int(DataFrame_Info.loc[DataFrame_Info.Set == 'Train', 'Size'].sum() * (1-train_val_split_ratio))\n",
    "embedding_dim = 128\n",
    "# Maximum Sequence Length\n",
    "Max_Seq_Length = 500\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "vectorize_layer = layers.experimental.preprocessing.TextVectorization(standardize = FilterData,\n",
    "                                                                      max_tokens = Max_Features,\n",
    "                                                                      output_mode = \"int\",\n",
    "                                                                      output_sequence_length = Max_Seq_Length)\n",
    "\n",
    "# seperating text from the labels\n",
    "text_ds = raw_train_ds.map(lambda x, y: x)\n",
    "# adapt: When this layer is adapted, it will analyze the dataset, determine the frequency of individual string values,\n",
    "# and create a 'vocabulary' from them\n",
    "vectorize_layer.adapt(text_ds)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E_nX4v_Po1tL"
   },
   "source": [
    "## Vectorize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0B_mkjW0o1tL"
   },
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    # adding an additional dimension with the last dimension index\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "\n",
    "# Vectorize the data.\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)\n",
    "\n",
    "# Buffering of the data for best performance on GPU. Basically, while the model is executing training step s,\n",
    "# the input pipeline is reading the data for step s+1\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=10)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KAdjQQ3jo1tN"
   },
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wWf3ixHno1tN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Keras_NLP\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 128)         114816    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 128)         114816    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,806,273\n",
      "Trainable params: 2,806,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# inputs\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# Embedding layer\n",
    "x = layers.Embedding(Max_Features, embedding_dim)(inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Conv1D + global max pooling\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "# a  hidden layer:\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Output layer\n",
    "predictions = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, predictions, name=\"Keras_NLP\")\n",
    "\n",
    "model.summary()\n",
    "plot_model(model, show_shapes=True, show_layer_names=False, expand_nested = True, rankdir = 'TB')\n",
    "\n",
    "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MbQWdVxjo1tP"
   },
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MtHMF4sho1tP"
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "# Fitting the model using the train and test datasets.\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=epochs)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_b18b8d7a_ac22_11ea_a00f_50e08586bf81row0_col0 {\n",
       "            background-color:  #fc7f00;\n",
       "            color:  #000000;\n",
       "        }    #T_b18b8d7a_ac22_11ea_a00f_50e08586bf81row0_col1 {\n",
       "            background-color:  #f7fcfd;\n",
       "            color:  #000000;\n",
       "        }    #T_b18b8d7a_ac22_11ea_a00f_50e08586bf81row0_col2 {\n",
       "            background-color:  #e4ff7a;\n",
       "            color:  #000000;\n",
       "        }    #T_b18b8d7a_ac22_11ea_a00f_50e08586bf81row0_col3 {\n",
       "            background-color:  #f7fcfd;\n",
       "            color:  #000000;\n",
       "        }    #T_b18b8d7a_ac22_11ea_a00f_50e08586bf81row1_col0 {\n",
       "            background-color:  #ffe217;\n",
       "            color:  #000000;\n",
       "        }    #T_b18b8d7a_ac22_11ea_a00f_50e08586bf81row1_col1 {\n",
       "            background-color:  #127c39;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b18b8d7a_ac22_11ea_a00f_50e08586bf81row1_col2 {\n",
       "            background-color:  #e8fc6c;\n",
       "            color:  #000000;\n",
       "        }    #T_b18b8d7a_ac22_11ea_a00f_50e08586bf81row1_col3 {\n",
       "            background-color:  #00441b;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b18b8d7a_ac22_11ea_a00f_50e08586bf81row2_col0 {\n",
       "            background-color:  #e4ff7a;\n",
       "            color:  #000000;\n",
       "        }    #T_b18b8d7a_ac22_11ea_a00f_50e08586bf81row2_col1 {\n",
       "            background-color:  #00441b;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b18b8d7a_ac22_11ea_a00f_50e08586bf81row2_col2 {\n",
       "            background-color:  #fc7f00;\n",
       "            color:  #000000;\n",
       "        }    #T_b18b8d7a_ac22_11ea_a00f_50e08586bf81row2_col3 {\n",
       "            background-color:  #006328;\n",
       "            color:  #f1f1f1;\n",
       "        }</style><table id=\"T_b18b8d7a_ac22_11ea_a00f_50e08586bf81\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" colspan=2>Train</th>        <th class=\"col_heading level0 col2\" colspan=2>Validation</th>    </tr>    <tr>        <th class=\"blank level1\" ></th>        <th class=\"col_heading level1 col0\" >Loss</th>        <th class=\"col_heading level1 col1\" >Accuracy</th>        <th class=\"col_heading level1 col2\" >Loss</th>        <th class=\"col_heading level1 col3\" >Accuracy</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_b18b8d7a_ac22_11ea_a00f_50e08586bf81level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_b18b8d7a_ac22_11ea_a00f_50e08586bf81row0_col0\" class=\"data row0 col0\" >0.4844</td>\n",
       "                        <td id=\"T_b18b8d7a_ac22_11ea_a00f_50e08586bf81row0_col1\" class=\"data row0 col1\" >0.7300</td>\n",
       "                        <td id=\"T_b18b8d7a_ac22_11ea_a00f_50e08586bf81row0_col2\" class=\"data row0 col2\" >0.3186</td>\n",
       "                        <td id=\"T_b18b8d7a_ac22_11ea_a00f_50e08586bf81row0_col3\" class=\"data row0 col3\" >0.8638</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b18b8d7a_ac22_11ea_a00f_50e08586bf81level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_b18b8d7a_ac22_11ea_a00f_50e08586bf81row1_col0\" class=\"data row1 col0\" >0.2174</td>\n",
       "                        <td id=\"T_b18b8d7a_ac22_11ea_a00f_50e08586bf81row1_col1\" class=\"data row1 col1\" >0.9158</td>\n",
       "                        <td id=\"T_b18b8d7a_ac22_11ea_a00f_50e08586bf81row1_col2\" class=\"data row1 col2\" >0.3221</td>\n",
       "                        <td id=\"T_b18b8d7a_ac22_11ea_a00f_50e08586bf81row1_col3\" class=\"data row1 col3\" >0.8762</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b18b8d7a_ac22_11ea_a00f_50e08586bf81level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_b18b8d7a_ac22_11ea_a00f_50e08586bf81row2_col0\" class=\"data row2 col0\" >0.1127</td>\n",
       "                        <td id=\"T_b18b8d7a_ac22_11ea_a00f_50e08586bf81row2_col1\" class=\"data row2 col1\" >0.9592</td>\n",
       "                        <td id=\"T_b18b8d7a_ac22_11ea_a00f_50e08586bf81row2_col2\" class=\"data row2 col2\" >0.4081</td>\n",
       "                        <td id=\"T_b18b8d7a_ac22_11ea_a00f_50e08586bf81row2_col3\" class=\"data row2 col3\" >0.8750</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x24af52d8188>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def Table_History(history):\n",
    "    Table = pd.DataFrame(pd.DataFrame(history.history).values,\n",
    "                     columns = pd.MultiIndex.from_product([['Train', 'Validation'], ['Loss', 'Accuracy']]))\n",
    "    display(Table.style.background_gradient(subset= [('Train', 'Accuracy'), ('Validation', 'Accuracy')], cmap='BuGn')\\\n",
    "            .background_gradient(subset= [( 'Train','Loss'), ('Validation', 'Loss')], cmap='Wistia').set_precision(4))\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    \n",
    "Table_History(history) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jBCQ3osAo1tT"
   },
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A string input\n",
    "inputs = tf.keras.Input(shape=(1,), dtype=\"string\")\n",
    "indices = vectorize_layer(inputs)\n",
    "outputs = model(indices)\n",
    "\n",
    "# Our end to end model\n",
    "final_model = tf.keras.Model(inputs, outputs)\n",
    "final_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "# Predictions\n",
    "Pred = final_model.predict(raw_test_ds)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# References\n",
    "1. Text classification from scratch, https://keras.io/examples/nlp/text_classification_from_scratch/\n",
    "1. Large Movie Review Dataset, https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "1. Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "text_classification_from_scratch",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
