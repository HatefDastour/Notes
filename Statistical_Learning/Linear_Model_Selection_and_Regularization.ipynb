{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['text.color'] = 'k'\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model Selection and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Variable Selection](#Variable-Selection)\n",
    "* [The Test Error](#The-Test-Error)\n",
    "    * [Indirect Estimatoins](#Indirect-Estimatoins)\n",
    "* [Shrinkage Methods](#Shrinkage-Methods)\n",
    "    * [Ridge Regression](#Ridge-Regression)\n",
    "    * [Lasso Regression](#Lasso-Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the linear regression model: $$Y= \\beta_0 + \\sum_{j=1}^{p}\\beta_{j} X_{j},$$ where $X_j$ represents the j-th predictor and $\\beta_j$ quantifies the association between that variable and the response.\n",
    "\n",
    "The linear regression model is commonly used to describe the relationship between a response Y and a set of variables $X_j,~1\\leq j\\leq p$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction Accuracy**: Provided that the true relationship between the response and the predictors is approximately linear, the least-squares estimates will have low bias.\n",
    "Let n denotes the number of observations and $p$ denotes the number of variables.\n",
    "\n",
    "* If $n \\gg  p$  ($n$ is much larger than $p$), least squares estimates tend to also have low variance and, thus, can perform well on test observations.\n",
    "\n",
    "* If $n$ is **not** much larger than $p$, then there can be a lot of variability in the least-squares fit, resulting in **overfitting** and consequently poor predictions on future observations not used in model training.\n",
    "\n",
    "* If $n<p$, then there is no longer a unique least squares coefficient estimate: the variance is infinite so the **method cannot be used at all**. \n",
    "\n",
    "The variance at the cost of a negligible increase in bias can be reduced significantly by **constraining** or **shrinking** the estimated coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Interpretability**: In many multiple regression models, several variables are not associated with the response. The resulting model can become more complex if such irrelevant variables are included.\n",
    "Including leads to. Nonetheless, removing these variables can lead to a model that is more easily interpreted. This task can be done by simply setting the corresponding coefficient estimates\n",
    "to zero! On the other hand, least-squares is extremely unlikely to yield any coefficient estimates that are exactly zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Selection\n",
    "There are three important classes of methods that can be used for **variable selection**\n",
    "\n",
    "* **Subset Selection**: a subset of the $p$ predictors is used for fitting a model using least squares.\n",
    "\n",
    "* **Shrinkage (regularization)**: A model is fitted using all $p$ predictors. However, the estimated coefficients are shrunken towards zero relative to the least-squares estimates.\n",
    "\n",
    "* **Dimension Reduction**. the $p$ predictors are projected into a M-dimensional subspace where $M < p$. Then these $M$ projections are used as predictors to fit a linear regression model by least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='Blue'>**Subset Selection**</font>:\n",
    "    * **Best Subset Selection**: All $\\left(\\begin{array}{c}p\\\\k \\end{array}\\right)$ models that contain exactly k predictors are fitted, and then a single best model is choosen.\n",
    "    * **Stepwise Selection**:\n",
    "        * <font color='Green'>**Forward Stepwise Selection**</font>: It begins with a model containing no predictors and then predictors are added to the model iteratively until all of the predictors are in the model. The variable that gives the greatest additional\n",
    "improvement to the fit is added to the model at each step.\n",
    "        * <font color='Green'>**Backward Stepwise Selection**</font>: Unlike forward stepwise selection, here the process begins  with the full least squares model containing all $p$ predictors, and then the least useful predictors are removed iteratively,\n",
    "        * <font color='Green'>**Hybrid Approaches**</font>: Variables are added to the model sequentially; however, after adding each new variable, the method may also remove any variables that no longer provide an improvement in the model fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Test Error\n",
    "Selecting the best model for the test error:\n",
    "\n",
    "* The test error can be **indirectly** estimated by adjusting the training error to account for the bias due to overfitting.\n",
    "* The test error can be **directly** estimated using either a validation set approach or a *cross-validation* approach.\n",
    "\n",
    "### Indirect Estimatoins\n",
    "Let $\\hat{\\sigma}^2$ be an estimate of the variance of the error terms. Then\n",
    "\\begin{align}\n",
    "C_p &= \\frac{1}{n} (RSS + 2d\\hat{\\sigma}^2 ),\\\\\n",
    "AIC &= \\frac{1}{n\\hat{\\sigma}^2}(RSS + 2d\\hat{\\sigma}^2),\\\\\n",
    "AIC &= \\frac{1}{n\\hat{\\sigma}^2}(RSS + \\log(n)d\\hat{\\sigma}^2),\\\\\n",
    "\\text{Adjusted }R^2 &= 1 -\\frac{RSS/(n − d − 1)}{TSS/(n − 1)}. \n",
    "\\end{align}\n",
    "\n",
    "### Direct Estimatoins\n",
    "Cross-validation used to be computationally prohibitive for many large problems; however, with recent advantages in computing, performing cross-validation is no longer an issue. Thus, cross-validation is a very attractive approach for selecting from among a number of models under consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrinkage Methods\n",
    "\n",
    "### Ridge Regression\n",
    "The main difference between least squares and this method is that the coefficients are estimated by minimizing a slightly different quantity.\n",
    "\n",
    "The ridge regression coefficient estimates $\\hat{\\beta}^R$ are the values that minimize\n",
    "\n",
    "$$\\underbrace{\\sum_{i=1}^{n}\\left(y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2}_{RSS}\n",
    "+ \\sum_{j=1}^{p} \\beta_j^2=RSS+\\sum_{j=1}^{p} \\beta_j^2$$\n",
    "where $\\lambda \\geq$ 0 is a *tuning parameter*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "The lasso coefficients, $\\hat{\\beta}^L_\\lambda$, minimize the quantity\n",
    "\n",
    "$$\\underbrace{\\sum_{i=1}^{n}\\left(y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2}_{RSS}\n",
    "+ \\sum_{j=1}^{p} |\\beta_j^2|=RSS+\\sum_{j=1}^{p} |\\beta_j^2|$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
